{
  "cells": [
    {
      "metadata": {
        "_uuid": "bc85b9dee6eaa2c34519ccc36362b051d280e1aa"
      },
      "cell_type": "markdown",
      "source": "## Objective\n\n* To predict the salary of a job based on the data posted on the company's website or on any job portal\n* To compare how addition of text(job description) into the models increases the prediction power"
    },
    {
      "metadata": {
        "_uuid": "ed476573d79f4cf8cd775ce52b0f14c28fd1af07"
      },
      "cell_type": "markdown",
      "source": "## About data\n\nI have used the job salary prediction dataset from Kaggle to perform the analysis and achieve the objective that i have mentioned above.\nFollowing is the link for the data description [Data Description](https://www.kaggle.com/c/job-salary-prediction/data)"
    },
    {
      "metadata": {
        "_uuid": "093b05cc12c963262913650c0dd57f6b37685781"
      },
      "cell_type": "markdown",
      "source": "## Approach\n\nRunning a classfier on the data is never the first step of any analysis be it on numerical data or text data. We will have to first understand the data, clean it, perform exploratory data analysis following which we will perform feature extraction and only then think of training a classifier on the training data.\n\nSo as a logical first step, i started with importing the data and performing exploratory data analysis on the data. For easy understanding of the entire analysis, i have listed all the steps that i have followed below and divide my entire analysis based in the same chronological order.\n\n1. Exploratory Data analysis\n2. Exploratory Data analysis - Job descriptions\n3. Models with just categorical variables\n4. Models with text variables\n5. Conclusion"
    },
    {
      "metadata": {
        "_uuid": "16735779da193e98cecd4f654a020bcb4198e1a1"
      },
      "cell_type": "markdown",
      "source": "### 1.Exploratory data analysis - general"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0b0d690e0b567e439100915e5804bad59a39f09"
      },
      "cell_type": "code",
      "source": "# Import all required modules for the analysis(make sure that you installed all these modules prior to importing)\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport matplotlib.pyplot as plt\nimport math\nfrom IPython.display import display,HTML\nfrom patsy import dmatrices\nimport seaborn as sns; sns.set()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n%pylab inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "905cf9ec2bb45a4cf3b8716253d8657361fd4276"
      },
      "cell_type": "code",
      "source": "# Reading the train data\ntrain_df = pd.read_csv('../input/Train_rev1.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84eebb30ede787a2e8a9944e2ba03adda1d61dab"
      },
      "cell_type": "code",
      "source": "# Checking the data types in the train data\nprint (train_df.info())\n\n# Let's look at the unique values present in the data frame to have a general understanding of the data\nnames = train_df.columns.values\nuniq_vals = {}\nfor name in names:\n    uniq_vals[name] = train_df.loc[:,name].unique()\n    print(\"Count of %s : %d\" %(name,uniq_vals[name].shape[0]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "dbf536c4d3d337459f1fdcfbbf549d6522a103ff"
      },
      "cell_type": "markdown",
      "source": "We can observe that the job descriptions are skewed mostly towards the lower end(mostly < 50,000) showing that most of the jobs are on the lower end of the job salary spectrum. This distribution might be useful as we move further into the analysis, as this might help us detect if there is any bias in our final analysis."
    },
    {
      "metadata": {
        "_uuid": "ced623332b3a001c9eb91f5fed7ca8ff702ad5f5"
      },
      "cell_type": "markdown",
      "source": "**As the dataset is huge with 250 k rows, we will first run various models on a very sample of 2500 rows and then use those models on the larger dataset**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a0b2c1b14b4a269377dfe93a36f06eb18c496e83"
      },
      "cell_type": "code",
      "source": "import random\nrandom.seed(1)\nindices = list(train_df.index.values)\nrandom_2500 = random.sample(indices,2500)\n\n# Subsetting the train data based on the random indices\ntrain_df1 = train_df.loc[random_2500].reset_index()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d8e3c5b3955edd356d9f49a6f7950e981112a8fe"
      },
      "cell_type": "markdown",
      "source": "Now let's see the log transfomration of the nomralized salaries in this data and compare them with the actual normalized salaries\nTaking log transformation will remove the skewness from the output variable and would give better results while predicting the outputs"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4eccd6ab2ecc6e9a1ac1a938e0056a9fff5f3ce5"
      },
      "cell_type": "code",
      "source": "fig,(ax1,ax2) = plt.subplots(1,2,figsize = (20,5))\n\nax1.hist(train_df1['SalaryNormalized'], bins='auto')\nax1.set_xlabel('Salaries')\nax1.set_ylabel('Number of postings')\nax1.set_title('Histogram of Salaries normalized')\n\nax2.hist(log(train_df1['SalaryNormalized']), bins='auto')\nax2.set_xlabel('Salaries')\nax2.set_ylabel('Number of postings')\nax2.set_title('Histogram of log of Salaries normalized')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9d1e9bf2d86312175de58f458f4e2aaef056fa6b"
      },
      "cell_type": "markdown",
      "source": "## 2. Exploratory Data analysis - Job descriptions\n\nLet's look into the job descriptions and try to answer some of the questions to have more clarity\n1. What are the top 5 parts of speech in the job descriptions? How frequently do they appear?\n2. How do these numbers change if you exclude stopwords?\n3. What are the 10 most common words after removing stopwords and lemmatization?"
    },
    {
      "metadata": {
        "_uuid": "6584c77cad215236c544f0174c10b1018e74e1ca"
      },
      "cell_type": "markdown",
      "source": "**1. What are the top 5 parts of speech in the job descriptions? How frequently do they appear?** "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "535e4afceea6170010b551561de74ce613186c3b"
      },
      "cell_type": "code",
      "source": "import re\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize, sent_tokenize \nstop_words = set(stopwords.words('english')) \nfrom string import punctuation\nimport collections",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "89196756d451b6ad2838af4614d5adafd3f73c35"
      },
      "cell_type": "markdown",
      "source": "While looking at the data, you can observe that the numbers are masked as *** and they turn out to be of no value for us in the analysis. In addition to that, there are a few data cleaning steps that i have performed in the below code\n1. Remove website links from the data\n2. Remove punctuations\n3. Removing numbers\n\nBy running these steps, we can achieve a higher accuracy as the data becomes more cleaned and the predictive power of the algorithm increases because of that."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bf1f76cf438655b40a52766056109f97af636e31"
      },
      "cell_type": "code",
      "source": "# To obtain the full width of a cell in a dataframe\npd.set_option('display.max_colwidth', -1)\ndesc = train_df1.loc[1,'FullDescription']\n\n# Creating a list of words from all the job descriptions in train_df1 data\nall_desc = []\nfor i in range(0,train_df1.shape[0]):\n    desc = train_df1.loc[i,'FullDescription']\n    desc1 = desc.lower()\n    # Removing numbers, *** and www links from the data\n    desc2 = re.sub('[0-9]+\\S+|\\s\\d+\\s|\\w+[0-9]+|\\w+[\\*]+.*|\\s[\\*]+\\s|www\\.[^\\s]+','',desc1)\n    # Removing punctuation\n    for p in punctuation:\n        desc2 = desc2.replace(p,'')\n    all_desc.append(desc2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2c3d3cf3a31427645d92cec7cdab24905fe74eb1"
      },
      "cell_type": "markdown",
      "source": "Now next step after cleaning the descriptions is to tokenize them. Here for simplicity purpose, i have just considered word tokenize. We can also tokenize the descriptions by sentences but that is not suitable for our problem here.\n[Refer this link](https://textminingonline.com/dive-into-nltk-part-ii-sentence-tokenize-and-word-tokenize) to know more about word tokenizer and sentence tokenizer"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d168ca2f0019996a9b64dcc1cffdccd348fac8f7"
      },
      "cell_type": "code",
      "source": "# Creating word tokens for all the descriptions\nfinal_list = []\nfor desc in all_desc:\n    word_list = word_tokenize(desc)\n    final_list.extend(word_list)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e67703c84c255242c50b4b564c414328d4c01b55"
      },
      "cell_type": "markdown",
      "source": "Now before finding the most common parts of speech in the job description, we first need to tag each word with the relevant parts of speech. I have used pos_tag from nltk library to tag the parts of speech. This uses the pos tags from treebank pos repository.   \nAfter creating the pos tags, the next would be to find the frequent occurence of each parts of speech and find the most common parts of speech"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1973bccad073d5c13ad53abe3e2d48275033fa1a"
      },
      "cell_type": "code",
      "source": "# 3. Tagging parts of speech\npos_tagged = nltk.pos_tag(final_list)\n\n# 4. Identifying the most common parts of speech\ntag_fd = nltk.FreqDist(tag for (word, tag) in pos_tagged)\ntag_fd.most_common()[:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f6854f2ccaca80680442dc20504d30bb263f6c07"
      },
      "cell_type": "markdown",
      "source": "You can see that Noun(NN) , adjective(JJ), preposition(IN), determiner(DT), plural nouns(NNS)  are the most common parts of speech from the job descriptions.\n[Refer this link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) to look at the descriptions of the parts of speech for the above result"
    },
    {
      "metadata": {
        "_uuid": "3361ef76d517d9bd1cd47846f76e504a85f33679"
      },
      "cell_type": "markdown",
      "source": "**2.How do these numbers change if you exclude stopwords?**  \nIn english,for that matter in any language, there will be a lot of stop words that repeat a lot of times in the sentence but do not actually carry a lot of information.  \nFor examples: the, a , an ,and etc.  \nnltk library has a repository for stop words. Let's remove those stopwords and check how the above results will change."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "550d379a426ef861dd08a8a9d979abb10b2edbef"
      },
      "cell_type": "code",
      "source": "# Excluding stopwords from the analysis\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english')) \n\nlist_wo_stopwords = []\nfor w in final_list:\n    if w not in stop_words:\n        list_wo_stopwords.append(w)\n        \n# 3. Tagging parts of speech\npos_tagged_wo_sw = nltk.pos_tag(list_wo_stopwords)\n\n# 4. Identifying the most common parts of speech\ntag_fd_wo_sw = nltk.FreqDist(tag for (word, tag) in pos_tagged_wo_sw)\ntag_fd_wo_sw.most_common()[:5]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bb181b2daec038ab906861e8df70223bd67c8200"
      },
      "cell_type": "markdown",
      "source": "After removing stopwords, there are two important observations in comparison to the previous result\n1. Prepositions and determiners disappeared from the top 5 set as most of these are present in the stopwords imported from NLTK \n2. The counts of nouns and plural nouns have decreased and the adjectives have increased.\n3. Verb, gerund or present participle(VBG) and Verb, non-3rd person singular present(VBP) moved to the top 5 list"
    },
    {
      "metadata": {
        "_uuid": "8e5ddce7d0193d94225ee41ff740bf8971d49935"
      },
      "cell_type": "markdown",
      "source": "**3. What are the 10 most common words after removing stopwords and lemmatization?**\n\nHere is some backgrond information about [lemmatization](https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization)\n\nAs we have already removed stopwords and create a dataframe list_wo_stopwords earlier, our first step here would be to perform lemmatization and then identify the 10 most common words.  \nI have also plotted the wordcloud of all the words to visualize these words.  "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7f1cd47e01ca74ddc5c35960a2c77eca287c7edc"
      },
      "cell_type": "code",
      "source": "from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\n# Lemmatization without specifying parts of speech\nlist_lemmatized = []\nfor word in list_wo_stopwords:\n    list_lemmatized.append(lemmatizer.lemmatize(word))\n\nword_freq_lem = dict(collections.Counter(list_lemmatized))\nkeys = list(word_freq_lem.keys())\nvalues = list(word_freq_lem.values())\ndf_lem = pd.DataFrame({'words':keys,'freq':values})\ndisplay(df_lem.sort_values(by = 'freq',ascending = False)[:10])\n\nfrom wordcloud import WordCloud\nfrom collections import Counter\nword_could_dict=Counter(word_freq_lem)\nwordcloud = WordCloud(width = 1000, height = 500).generate_from_frequencies(word_could_dict)\n\nplt.figure(figsize=(15,8))\nplt.imshow(wordcloud)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a4f455542b2322b9ef14500146a87476a21bf408"
      },
      "cell_type": "markdown",
      "source": "## 3. Model without text variables\n\nAs i have told, we will perform the analysis with 2 types of variables\n1. Non-text variables\n2. Text variables\n\nJust to revisit, our objective is to predict the salaries based on the information posted on the website including variables such as location, company and job description.\n\nWith this in mind, let's proceed with the analysis\n\nFor this analysis, let's define the target variable based on the salary normalized. This converts the problem into a classfication problem and reduces the complexity. Further, based on the requirement we can perform a regression analysis to predict a number for the salary"
    },
    {
      "metadata": {
        "_uuid": "25ada3c31721fdc17e1771f30470c6555f200a27"
      },
      "cell_type": "markdown",
      "source": "**Creating the target column by splitting the salary normalized into high(above 75th percentile) and low(below 75th percentile)**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cee9c4a60c36f7761955f225bbc57898ed5a486c"
      },
      "cell_type": "code",
      "source": "p_75 = np.percentile(train_df1['SalaryNormalized'], 75)\ntrain_df1['target'] = train_df1['SalaryNormalized'].apply(lambda x: 1 if x>=p_75 else 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "706a180c53f0f6bb5404c60e7bc92d933e3b9611"
      },
      "cell_type": "markdown",
      "source": "### Creation of features\n**Creating a proxy variable for location**\nHere as there are so many locations, creating dummy variable will bloat the dataset a lot. So, we will create a proxy variable\nfor the location by taking the cities with high cost of living under one group(1) and the others in a separate group(0).\n\nWe have considered 17 cities as cities with high cost of living [Source](https://www.thisismoney.co.uk/money/mortgageshome/article-5283699/The-cheapest-expensive-cities-live-in.html)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fa8db59adf43e5e965e468e1b31846a3d4d81214"
      },
      "cell_type": "code",
      "source": "costly_cities = ['London','Brighton','Edinburgh','Bristol','Southampton','Portsmouth','Exeter','Cardiff','Manchester',\n                 'Birmingham','Leeds','Aberdeen','Glasgow','Newcastle','Sheffield','Liverpool']\ncostly_cities_lower = [x.lower() for x in costly_cities]\n\ntrain_df1['location_flag'] = train_df1['LocationNormalized'].apply(lambda x: 1 if x in costly_cities else 0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83aa3b2c62255ee36709556fa0419b33f79340d8"
      },
      "cell_type": "markdown",
      "source": "**Creating dummy variables for all the columns except for job descriptions and splitting the data into train and validation set**  "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56de84ec190cc07658e84e23272d04030e8dbf02"
      },
      "cell_type": "code",
      "source": "# Dropping job description column from the dataset\ntrain_x = train_df1.drop(['FullDescription','index','Id','LocationRaw','Title','Company','LocationNormalized','SalaryRaw','SalaryNormalized',\n                    'target'],axis=1)\n\ntrain_x1 = pd.get_dummies(train_x,drop_first=True)\nX_n = np.array(train_x1)\ny_n = np.array(train_df1['target'])\n\nfrom sklearn.model_selection import train_test_split\nX_train_num, X_val_num, y_train_num, y_val_num = train_test_split(X_n, y_n, test_size=0.3, random_state=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f467e0dda356b58c19460b2c7db25bc5d85fb7cb"
      },
      "cell_type": "markdown",
      "source": "**Let's run a Bernoulli Naive Bayes algorithm to predict the salary of a job using the non-text variables. I have selected this algorithm because most often Naive bayes gives good results in simplistic scenarios.  \nIt also acts as a good starting point to compare the effect of non-text and text variables on prediction.**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6771d320cb56044ea6ee8f0232089f4e0c125c47"
      },
      "cell_type": "code",
      "source": "# Bernoulli\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB()\nclf.fit(X_train_num, y_train_num)\n\nfrom sklearn import metrics\nprediction_train = clf.predict(X_val_num)\nmat_n = metrics.confusion_matrix(y_val_num, prediction_train)\nmat_n\nprint (metrics.accuracy_score(y_val_num, prediction_train))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5a874e95d5ce1e5ef9de097fab3b947baef859ed"
      },
      "cell_type": "markdown",
      "source": "**Using test data to predict the salary**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "722bf263f14fff9907ddeb587b132c98ada86b96"
      },
      "cell_type": "code",
      "source": "# Baseline accuracy\n1-(sum(y_val_num)/len(y_val_num))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cd27e2cf7b1a57b726d1833abf78057911869467"
      },
      "cell_type": "markdown",
      "source": "You can see that we did not very well with just numeric values in the model as the accuracy is almost equal to the baseline accuracy. Now let's look at the model with just the job descriptions and see if we can predict the salaries with some higher accuracies"
    },
    {
      "metadata": {
        "_uuid": "79dfb3df9544e7bb7019faf06106c8e615d13684"
      },
      "cell_type": "markdown",
      "source": "## 4. Models using text as variables\n\nNow let's run the Naive Bayes using the words in the job description as variables. You will notice that text in itself has a higher prediction power than the non-text variables\n\nHere i have followed a bag-of-words model for creating the features for the model\n\nFor this model, we will run both Bernoulli and Multinomial Naive Bayes([click to know more](https://syncedreview.com/2017/07/17/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation/)), to check the performance of both the models\n\nWe will run both the models in multiple steps\n1. Without removing stopwords from the data\n2. After removing stopwords from the data\n3. After lemmatizing the data\n\nThis will help us understand the effect of each step on the accuracy of the result\n\nHere i have created a function to make it easy to perform the above 3 steps"
    },
    {
      "metadata": {
        "_uuid": "7f5617f6b129c4c952d4c27bd9c034a453b15cc0"
      },
      "cell_type": "markdown",
      "source": "### Naive Bayes models( Bernoulli and Multinomial)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81b0e35c9a7bd64f9cdbfd3b85de3e9e1819122b"
      },
      "cell_type": "code",
      "source": "def naive_bayes_models(l):\n    # Counting the occurence of each word in the corpus\n    from sklearn.feature_extraction.text import CountVectorizer\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(l)\n    count_vect.get_feature_names()\n    X_matrix= X_train_counts.todense()\n\n    y = np.array(train_df1['target'])\n\n    # Creating the train and test split\n    from sklearn.model_selection import train_test_split\n    X_train_m, X_val_m, y_train_m, y_val_m = train_test_split(X_train_counts, y, test_size=0.3, random_state=1)\n\n    #Multinomial\n\n    from sklearn.naive_bayes import MultinomialNB\n    clf_m = MultinomialNB().fit(X_train_m, y_train_m)\n    labels_m = clf_m.predict(X_val_m)\n\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    mat_m = confusion_matrix(y_val_m, labels_m)\n\n    # Bernoulli\n    # Changing the data to binary to input BernoulliNB\n    x_train_b1 = X_train_counts.todense()\n    X_train_counts_ber = np.where(x_train_b1 >=1 ,1,0)\n\n    # Creating the train and test split for bernoulli\n    from sklearn.model_selection import train_test_split\n    X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_train_counts_ber, y, test_size=0.3, random_state=1)\n\n    from sklearn.naive_bayes import BernoulliNB\n    clf_b = BernoulliNB().fit(X_train_b, y_train_b)\n    labels_b = clf_b.predict(X_val_b)\n\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    mat_b = confusion_matrix(y_val_b, labels_b)\n    print ('Confusion matrix:',mat_b)\n    print ('Accuracy using BernoulliNB:',accuracy_score(y_val_b, labels_b))\n    \n\n    print ('Confusion matrix:',mat_m)\n    print ('Accuracy using MultinomialNB:',accuracy_score(y_val_m, labels_m))\n    test_mse = metrics.mean_squared_error(clf_m.predict(X_val_m), y_val_m)\n    print('test_mse :', test_mse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b89a6a3290271c3cb4e06c122a287c0804eb99cb"
      },
      "cell_type": "markdown",
      "source": "**1. Without removing stopwords**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "279451bac3d0f2f788209e6a9e4b511d03ab281f"
      },
      "cell_type": "code",
      "source": "naive_bayes_models(all_desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c706598eba95b992c5750a56e2a5b61c63cb63c4"
      },
      "cell_type": "markdown",
      "source": "**2. After removing stopwords from the data**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56ed56e83070d06e903126dacf32a4997acde5b7"
      },
      "cell_type": "code",
      "source": "# Removing stopwords\ndef remove_stopwords(s):\n    big_regex = re.compile(r'\\b%s\\b' % r'\\b|\\b'.join(map(re.escape, stop_words)))\n    return big_regex.sub('',s)\n\nall_desc_wo_sw = [remove_stopwords(s) for s in all_desc]\nnaive_bayes_models(all_desc_wo_sw)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "00eb31e890a66b5bd56f56a9c7530fdbebed5d59"
      },
      "cell_type": "markdown",
      "source": "**3. After lemmatizing the data**\n\nLemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.([Click to know more](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html))\n\nWe will lemmatize and see if there is any improvement in the result"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f830734a6274ba1d3c2e41aaa25949b770e2252f"
      },
      "cell_type": "code",
      "source": "from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\nfrom nltk.corpus import wordnet\n\ndef get_wordnet_pos(treebank_tag):\n\n    if treebank_tag.startswith('J'):\n        return wordnet.ADJ\n    elif treebank_tag.startswith('V'):\n        return wordnet.VERB\n    elif treebank_tag.startswith('N'):\n        return wordnet.NOUN\n    elif treebank_tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return None \n\n# Lemmatizing the data\nall_desc_lemm = []\nfor i in range(0,len(all_desc_wo_sw)):\n    desc = all_desc_wo_sw[i]\n    desc2 = re.sub('[0-9]+\\S+|\\s\\d+\\s|\\w+[0-9]+|\\w+[\\*]+.*|\\s[\\*]+\\s|www\\.[^\\s]+','',desc)\n    for p in punctuation:\n        desc2 = desc2.replace(p,'')\n    tagged = nltk.pos_tag(word_tokenize(desc2))\n    list_lemmatized = []\n    for word, tag in tagged:\n        wntag = get_wordnet_pos(tag)\n        if wntag is None:# not supply tag in case of None\n            list_lemmatized.append(lemmatizer.lemmatize(word)) \n        else:\n            list_lemmatized.append(lemmatizer.lemmatize(word, pos=wntag))\n    k = ' '.join(list_lemmatized)   \n    all_desc_lemm.append(k)\n\nnaive_bayes_models(all_desc_lemm)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "763f3b2c435defa9750c15145db3b1ca92c91afa"
      },
      "cell_type": "markdown",
      "source": "### Random forest"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c581cef912d7abb9102c09fea02b23a3260ca9a0"
      },
      "cell_type": "code",
      "source": "def random_forest(l):\n    # Counting the occurence of each word in the corpus\n    from sklearn.feature_extraction.text import CountVectorizer\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(l)\n    count_vect.get_feature_names()\n    X_matrix= X_train_counts.todense()\n\n    y = np.array(train_df1['target'])\n\n    # Creating the train and test split\n    from sklearn.model_selection import train_test_split\n    X_train_m, X_val_m, y_train_m, y_val_m = train_test_split(X_train_counts, y, test_size=0.3, random_state=1)\n    \n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import GridSearchCV\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import accuracy_score\n    model = RandomForestClassifier(n_estimators = 250,max_features = None).fit(X_train_m, y_train_m)\n#     grid_params = {'criterion': ['gini'], 'max_features' : [None], 'n_estimators': [300]}\n#     para_search = GridSearchCV(model, grid_params, scoring = 'accuracy', cv = 5).fit(X_train_m, y_train_m)\n#     best_model = para_search.best_estimator_\n    labels_r = model.predict(X_val_m)\n    \n    train_mse = metrics.mean_squared_error(model.predict(X_train_m), y_train_m)\n    test_mse = metrics.mean_squared_error(labels_r, y_val_m)\n    print(train_mse)\n    print(test_mse)\n    print ('Accuracy using Random Forest:',accuracy_score(y_val_m, labels_r))\n    mat_r = confusion_matrix(y_val_m, labels_r)\n    print(mat_r)\n    return model\n\nbest_model1 = random_forest(all_desc)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "147d1f67351697afe1c84ec8cdef39fa80500915"
      },
      "cell_type": "code",
      "source": "best_model1 = random_forest(all_desc_lemm)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7dc3cc2bec336945c90e7503e3b08342d0b91afc"
      },
      "cell_type": "markdown",
      "source": "Need to change the representation for showing the results from different models"
    },
    {
      "metadata": {
        "_uuid": "6ae7c74d42d420fed22edc551d42ac1b7eeb7242"
      },
      "cell_type": "markdown",
      "source": "## 5. Conclusion\n"
    },
    {
      "metadata": {
        "_uuid": "e0a4e38a132f044b6104cfc95a9e2deac2766804"
      },
      "cell_type": "markdown",
      "source": "The bag-of-words model used is the simplest possible feature space. Other natural language processing\ntechniques could help, such as removing stop words and using longer strings of words as\nfeatures such as bigrams and trigrams. Another appealing option would be to use simple syntatic\nanalysis to extract noun phrases such as “heavy machinery technician” from the description."
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "1f82227673fcffc2749fd7b0fa2c13fabe1b4035"
      },
      "cell_type": "markdown",
      "source": "You can see the accuracy has increased from 80% in the first step to 82% in the lemmatization while using text data. There is a still a lot of scope for this analysis. We can use SVM algorithm to predict the salary based on the text data. In the next version of this analysis, i will try more models and compare them with the results from the current analysis. Hope you enjoyed the analysis. Please share your thoughts."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}